<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="InsightBench">
  <meta name="keywords" content="Analytics Agent">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FM2DS</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">FM<sup>2</sup>DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering</h1>
            <div class="is-size-5 publication-authors">
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                        <a href="https://amirabaskohi.github.io/" style="color:#f68946;font-weight:normal;">Amirhossein Abaskohi<sup> 1,2 </sup></a> ,
                    </span>
                    <span class="author-block">
                        <a href="https://www.servicenow.com/research/author/spandana-gella.html" style="color:#f68946;font-weight:normal;">Spandana Gella<sup> 1 </sup></a> ,
                    </span>
                    <span class="author-block">
                        <a href="https://www.cs.ubc.ca/~carenini/" style="color:#f68946;font-weight:normal;">Giuseppe Carenini<sup> 2 </sup></a> ,
                    </span>
                    <span class="author-block">
                        <a href="https://www.servicenow.com/research/author/issam-h.-laradji.html" style="color:#F2A900;font-weight:normal;">Issam Hadj Laradji<sup> 1,2 </sup></a> 
                    </span>                    
                </div>
                <div class="is-size-5 publication-authors">
                    <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b>ServiceNow Research <sup> 1 </sup></span>
                    <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b>UBC, Vancouver, Canada <sup> 2 </sup></span>
                </div>
                <div class="column has-text-centered">
                    <div class="publication-links">
                <span class="link-block">
                    <a href="https://www.arxiv.org/abs/2412.07030" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                </span>
                <span class="link-block">
                    <a href="https://github.com/ServiceNow/FM2DS" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/ServiceNow/FM2DS/blob/main/M2QA_Bench.json" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>
            </div>
            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="" style="color:#f68946;font-weight:normal;"><sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="" style="color:#008AD7;font-weight:normal;"><sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="" style="color:#F2A900;font-weight:normal;"></a>,
              </span>
              <span class="author-block">
                <a href="" style="color:#f68946;font-weight:normal;"></a>
              </span>
            </div> -->

            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> </b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> </span>
              <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b></span>
            </div> -->

            <!-- <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
            </div> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section" style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <h2 class="title is-3" style="text-align: center;"></h2>
      <!-- Image -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <figure>
            <img src="static/images/teaser.png" alt="Teaser Image">
            <figcaption> <b>Figure 1:</b> In contrast to traditional datasets that depend on human annotators, 
              templates, and information snippets as sources, FM<sup>2</sup>DS is a fully automated approach that utilizes complete documents as its sources.
              FM<sup>2</sup>DS incorporates validation steps to ensure that the generated questions are answerable, multimodal, and multihop.</figcaption>
          </figure>
          <figure>
            <img src="static/images/FM2DS.png" alt="Teaser Image">
            <figcaption> <b>Figure 2:</b> The Five-Stage Pipeline for FM<sup>2</sup>DS. First we retrieve relevant documents from the Wikipedia dataset to create a pool of 
              related documents based on hyperlinks and topics (Stage 1). In Stage2, we select the few-shot samples from MultiModalQA (MMQA in the figure).
              Stage 3 focuses on generating and validating questions to make sure they are answerable, multihop, and multimodal . 
              In Stage 4, answers are generated and validated. Finally, in Stage 5 we generate queries related to the documents, which are also validated to ensure relevance and accuracy.</figcaption>
          </figure>
        </div>
      </div>
      <!-- Abstract -->
      <div class="columns is-centered has-text-centered">
        <div class="columns is-centered has-text-centered">
            <div class="column is-six-fifths">
              <div class="content has-text-justified">
                <p>
                  Multimodal multihop question answering is a complex task that requires reasoning over multiple sources of information, such as images and text, to answer questions. 
While there has been significant progress in visual question answering, the multihop setting remains unexplored due to the lack of high-quality datasets.
Current methods focus on single-hop question answering or a single modality, which makes them unsuitable for real-world scenarios such as analyzing multimodal educational materials, summarizing lengthy academic articles, or interpreting scientific studies that combine charts, images, and text. 
To address this gap, we propose a novel methodology, introducing the first framework for creating a high-quality dataset that enables training models for multimodal multihop question answering.  
Our approach consists of a 5-stage pipeline that involves acquiring relevant multimodal documents from Wikipedia, synthetically generating high-level questions and answers, and validating them through rigorous criteria to ensure quality data. 
We evaluate our methodology by training models on our synthesized dataset and testing on two benchmarks, our results demonstrate that, with an equal sample size, models trained on our synthesized data outperform those trained on human-collected data by 1.9 in exact match (EM) on average.  
We believe our data synthesis method will serve as a strong foundation for training and evaluating multimodal multihop question answering models.
                </p>
              </div>
            </div>
          </div>
      </div>
  </section>

  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">M<sup>2</sup>QA</h2>
      </div>
    </div>
    <!-- </div> -->
  <div class="container is-max-desktop">
  
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified"> 
            <figure>
                <img src="static/images/M2QA_sample.png" alt="Teaser Image">
                <figcaption> <b>Figure 3:</b> Multimodal multihop reasoning example from  M<sup>2</sup>QA where the model compares the release dates of two albums,
                  "Music from Big Pink" and "Imagine," using textual and visual cues. The documents are connected through their shared topic, "music,"
                  and the answer is determined as the title of the earlier-released album.</figcaption>
              </figure>
            </div>
          </div> 
    </section>
    <section class="section">
        <div class="container">
          <div class="columns is-centered has-text-centered">
            <div class="column is-three-quarters">
              <div class="content has-text-justified">
                <p>
                  We also propose a benchmark, M<sup>2</sup>QA, to assess the LVLMs performance on a more complicated MMQA task with full documents.
                  M<sup>2</sup>QA consists of 500 Q&A pairs, each designed to challenge the model's ability to perform a complex reasoning task.
                  The questions are not templated into a specific structure (as in some existing works like MultimodalQA), instead, they are diverse and challenging.
                  Additionally, answering the questions require access to full documents, where both information extraction and reasoning across different modalities (e.g., images and tables) are essential. 
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>
      
      
  
<!-- <section class="hero teaser">
<div class="container is-max-desktop">
    <div class="hero-body">
    <h4 class="subtitle has-text-centered">
        &#x1F51C;<span style="color: #ff3860">[Coming Soon!]</span> More details about dataset creation, experimental setup, and evaluation!
    </h4>
    </div>
</div>
</section> -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{abaskohi2024fm2dsfewshotmultimodalmultihop,
      title={FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering}, 
      author={Amirhossein Abaskohi and Spandana Gella and Giuseppe Carenini and Issam H. Laradji},
      year={2024},
      eprint={2412.07030},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.07030}, 
}/code></pre>
    </div>
</section>
<!--End BibTex citation -->

</body>

</html>
